{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lemmatization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 09-02-2022"
      ],
      "metadata": {
        "id": "xfi9zvD2FjcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njc55VFFGTnj",
        "outputId": "8cbd92d3-b664-4c47-dd32-7a4ac705c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_dzPrgdFf2a"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"She jumped inot the river and breathed heavily\"\n",
        "wordnet = WordNetLemmatizer()\n",
        "tokenizer = word_tokenize(text)\n",
        "\n",
        "for token in tokenizer:\n",
        "  print(token,\"-->\", wordnet.lemmatize(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NeRESpLFtY9",
        "outputId": "986a9e51-2d5c-4680-e295-ef5e45b01d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She --> She\n",
            "jumped --> jumped\n",
            "inot --> inot\n",
            "the --> the\n",
            "river --> river\n",
            "and --> and\n",
            "breathed --> breathed\n",
            "heavily --> heavily\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am running and I usually use don't drink water in between\"\n",
        "wordnet = WordNetLemmatizer()\n",
        "tokenizer = word_tokenize(text)\n",
        "\n",
        "for token in tokenizer:\n",
        "  print(token,\"-->\", wordnet.lemmatize(token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JCXUSKSGwnL",
        "outputId": "ea5de060-50a1-4920-80dc-f75b24a67a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I --> I\n",
            "am --> am\n",
            "running --> running\n",
            "and --> and\n",
            "I --> I\n",
            "usually --> usually\n",
            "use --> use\n",
            "do --> do\n",
            "n't --> n't\n",
            "drink --> drink\n",
            "water --> water\n",
            "in --> in\n",
            "between --> between\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,pos_tag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7FvrjoLG_aQ",
        "outputId": "79793e31-2a02-4bcd-f2aa-6ba158542023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"She jumped inot the river and breathed heavily\"\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "for token,tag in pos_tag(word_tokenize(text)):\n",
        "  pos=tag[0].lower()\n",
        "\n",
        "  if pos not in ['a','r','n','v']:\n",
        "    pos='n'\n",
        "  print(token,\"-->\", wordnet.lemmatize(token,pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU5xWmsqHtrB",
        "outputId": "aa4394ad-a6b1-47dd-ddd1-7647fcb10f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She --> She\n",
            "jumped --> jump\n",
            "inot --> inot\n",
            "the --> the\n",
            "river --> river\n",
            "and --> and\n",
            "breathed --> breathe\n",
            "heavily --> heavily\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am running and I usually use to runs\"\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "for token,tag in pos_tag(word_tokenize(text)):\n",
        "  pos=tag[0].lower()\n",
        "\n",
        "  if pos not in ['a','r','n','v']:\n",
        "    pos='n'\n",
        "  print(token,\"-->\", wordnet.lemmatize(token,pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLZnxrMuIP5D",
        "outputId": "414ee5c1-89b1-49a5-e027-b2090841a621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I --> I\n",
            "am --> be\n",
            "running --> run\n",
            "and --> and\n",
            "I --> I\n",
            "usually --> usually\n",
            "use --> use\n",
            "to --> to\n",
            "runs --> run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer,PorterStemmer,WordNetLemmatizer\n",
        "from nltk import word_tokenize,pos_tag\n",
        " \n",
        "snowball = SnowballStemmer(language='english')\n",
        "porter = PorterStemmer()\n",
        "wordnet = WordNetLemmatizer()\n",
        " \n",
        "text = [\"better\",\"Caring\",\"are\",\"am\",\"worse\",\"struggling\",'meeting']\n",
        "print(\"{0:10}{1:20}{2:30}\".format(\"Word\",\"Snowball Stemmer\",\"Wordnet Lemmatizer\"))\n",
        "for token,tag in pos_tag(text):\n",
        "\n",
        "    pos=tag[0].lower()\n",
        "    if pos not in ['a', 'r', 'n', 'v']:\n",
        "        pos='n'\n",
        "\n",
        "    print(\"{0:10}{1:20}{2:30}\".format(token,snowball.stem(token),wordnet.lemmatize(token,pos)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXL42nZZJcFK",
        "outputId": "e0e5ff18-128a-469f-d1d9-72010195f310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word      Snowball Stemmer    Wordnet Lemmatizer            \n",
            "better    better              well                          \n",
            "Caring    care                Caring                        \n",
            "are       are                 be                            \n",
            "am        am                  be                            \n",
            "worse     wors                worse                         \n",
            "strugglingstruggl             struggle                      \n",
            "meeting   meet                meeting                       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string  = \"Books! And cleverness! There are more important things - friendship and bravery and - oh Harry - be careful!\"\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "sequences = sent_tokenize(string)\n",
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqbkTCa2KCvg",
        "outputId": "8ca9e735-97bc-4413-e7dd-62c74fe7fbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Books!',\n",
              " 'And cleverness!',\n",
              " 'There are more important things - friendship and bravery and - oh Harry - be careful!']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_tokens = [word_tokenize(seq) for seq in sequences]\n",
        "seq_tokens "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKyw1VY-KlMq",
        "outputId": "0b7f838d-9f01-472c-d3b6-adc4438a9bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Books', '!'],\n",
              " ['And', 'cleverness', '!'],\n",
              " ['There',\n",
              "  'are',\n",
              "  'more',\n",
              "  'important',\n",
              "  'things',\n",
              "  '-',\n",
              "  'friendship',\n",
              "  'and',\n",
              "  'bravery',\n",
              "  'and',\n",
              "  '-',\n",
              "  'oh',\n",
              "  'Harry',\n",
              "  '-',\n",
              "  'be',\n",
              "  'careful',\n",
              "  '!']]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "no_punct_seq_tokens=[]\n",
        "for seq_token in seq_tokens:\n",
        "  no_punct_seq_tokens.append([\n",
        "                              token for token in seq_token \n",
        "                              if token not in string.punctuation\n",
        "                              ])"
      ],
      "metadata": {
        "id": "jqOL9JP6LQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_punct_seq_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQIkP6icL-uv",
        "outputId": "361b9360-cc86-46f7-9a8d-fc6c655e2d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Books'],\n",
              " ['And', 'cleverness'],\n",
              " ['There',\n",
              "  'are',\n",
              "  'more',\n",
              "  'important',\n",
              "  'things',\n",
              "  'friendship',\n",
              "  'and',\n",
              "  'bravery',\n",
              "  'and',\n",
              "  'oh',\n",
              "  'Harry',\n",
              "  'be',\n",
              "  'careful']]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "lm = WordNetLemmatizer()\n",
        "lemm_tokens = [lm.lemmatize(token) for seq in no_punct_seq_tokens for token in seq]"
      ],
      "metadata": {
        "id": "KxqBHHZvMYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemm_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9BJaQPNM8e6",
        "outputId": "5a159fef-814f-402f-d655-58653f2df117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Books',\n",
              " 'And',\n",
              " 'cleverness',\n",
              " 'There',\n",
              " 'are',\n",
              " 'more',\n",
              " 'important',\n",
              " 'thing',\n",
              " 'friendship',\n",
              " 'and',\n",
              " 'bravery',\n",
              " 'and',\n",
              " 'oh',\n",
              " 'Harry',\n",
              " 'be',\n",
              " 'careful']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stem_tokens = [stemmer.stem(token) for seq in no_punct_seq_tokens for token in seq]\n",
        "stem_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05zBIIqQNYCn",
        "outputId": "9ef6f66e-9b76-4aa9-e390-962013279b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book',\n",
              " 'and',\n",
              " 'clever',\n",
              " 'there',\n",
              " 'are',\n",
              " 'more',\n",
              " 'import',\n",
              " 'thing',\n",
              " 'friendship',\n",
              " 'and',\n",
              " 'braveri',\n",
              " 'and',\n",
              " 'oh',\n",
              " 'harri',\n",
              " 'be',\n",
              " 'care']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatized tokens : \",lemm_tokens)\n",
        "print(\"Stem tokens : \",stem_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8p7pJwtOS1y",
        "outputId": "387d5fc2-520e-4ace-f28b-bc9b77a1756d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized tokens :  ['Books', 'And', 'cleverness', 'There', 'are', 'more', 'important', 'thing', 'friendship', 'and', 'bravery', 'and', 'oh', 'Harry', 'be', 'careful']\n",
            "Stem tokens :  ['book', 'and', 'clever', 'there', 'are', 'more', 'import', 'thing', 'friendship', 'and', 'braveri', 'and', 'oh', 'harri', 'be', 'care']\n"
          ]
        }
      ]
    }
  ]
}